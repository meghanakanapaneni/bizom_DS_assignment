{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN7d_35UQcKK"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYMlaWJqzv5G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djiHvgydQevl"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRSUCV11QOfg"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import glob\n",
        "from csv import writer\n",
        "import csv\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import Image as ig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21VkHNYwAsA"
      },
      "source": [
        "Count of Train and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is-WzuS1oXMD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train='/content/gdrive/My Drive/Mobisy/trainingSet/train/'\n",
        "valid='/content/gdrive/My Drive/Mobisy/trainingSet/val/'\n",
        "sum_t=0\n",
        "sum_v=0\n",
        "for j in range(0,10):\n",
        "  sum_t=sum_t+len([entry for entry in os.listdir(train+str(j)) if os.path.isfile(os.path.join(train+str(j), entry))])\n",
        "  sum_v=sum_v+len([entry for entry in os.listdir(valid+str(j)) if os.path.isfile(os.path.join(valid+str(j), entry))])\n",
        "print(sum_t,sum_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUoi1G_VO_fj"
      },
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obC3pAb7wMXs"
      },
      "source": [
        "Data Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qoaeq1nl8rwe"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/gdrive/My Drive/Mobisy/trainingSet/'\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.Grayscale(1),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Grayscale(1),\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRh_ycQW-SB2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZYu-FXOOVNQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_datasets = {x: datasets.ImageFolder(root=os.path.join(data_path, x),transform=data_transforms[x]) for x in ['train', 'val']}\n",
        "print(image_datasets)\n",
        "dataloaders = {x:torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True) for x in ['train', 'val']}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTVphcExwSqD"
      },
      "source": [
        "Load dataset and apply dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL_gwwt8-Yto"
      },
      "outputs": [],
      "source": [
        "target = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_jMRiBA-cR9"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = {x:len(image_datasets[x]) for x in ['train', 'val']}\n",
        "dataset_sizes['val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Hvxyozg9Nm"
      },
      "outputs": [],
      "source": [
        "def create_torch_tensor(input):\n",
        "  second_input = torch.as_tensor(input).to(device)\n",
        "  second_input=second_input.float()\n",
        "  second_output = torch.sum(second_input, axis=-1).reshape(-1, 1)  # (N, C)\n",
        "  return second_input,second_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_1XFaHjwZbz"
      },
      "source": [
        "Model 1 - Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPyqRBI5-nnN"
      },
      "outputs": [],
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1) \n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)                                  \n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2)                               \n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1)                                  \n",
        "        self.fc1 = nn.Linear(8*11*11, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.conv1(x)))         # output: 26*26 -> # output: 25*25\n",
        "        x = self.pool2(self.relu(self.conv2(x)))         # output:  12*12 -> 11*11\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net1 = Model1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHOcLVFwu3g"
      },
      "source": [
        "Model 2 - Summation of 2 numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3HvqD7fxMyl"
      },
      "outputs": [],
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2, self).__init__()                             \n",
        "        self.fc1 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "net2 = Model2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yti5vNVpweu9"
      },
      "source": [
        "Building Methods to train model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BJmp56Q--dM"
      },
      "outputs": [],
      "source": [
        "class Methods:\n",
        "    def __init__(self):\n",
        "        self.model1 = None\n",
        "        \n",
        "    def train(self, model1,model2, epochs1,epochs2, optimizer1,optimizer2, criterion1,criterion2):\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        start = time.time()\n",
        "        for epoch in (range(epochs1)):  \n",
        "            print(f'Epoch: {epoch+1}')\n",
        "            for mode in ['train', 'val']:\n",
        "                running_loss_m1 = 0.0\n",
        "                running_acc_m1 = 0.0\n",
        "                running_loss_m2 = 0.0\n",
        "                running_acc_m2 = 0.0\n",
        "                for i, data in tqdm(enumerate(dataloaders[mode])):\n",
        "                    inputs, labels = data\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    inputs = inputs.float()\n",
        "                    optimizer1.zero_grad()\n",
        "                    outputs = self.model1(inputs)\n",
        "                    loss1 = criterion1(outputs, labels)\n",
        "\n",
        "                    if mode=='train':\n",
        "                      \n",
        "                        labels_in_process=labels.tolist()\n",
        "                        input_data_processed=[]\n",
        "                        for m in range(0,len(labels_in_process)):\n",
        "                          input_data_processed.append([labels_in_process[m],random.randint(0,9)])\n",
        "                        second_input,second_output=create_torch_tensor(input_data_processed)\n",
        "                        y_pred = model2(second_input) \n",
        "                        loss2 = criterion2(y_pred, second_output)  \n",
        "                        loss2.backward()  \n",
        "                        optimizer2.step() \n",
        "                        optimizer2.zero_grad() \n",
        "              \n",
        "                        loss1.backward()\n",
        "                        optimizer1.step()\n",
        "\n",
        "               \n",
        "                    running_loss_m1 += loss1.item()\n",
        "                    running_loss_m2 += loss2.item()\n",
        "                \n",
        "                    outputs = torch.log_softmax(outputs, dim=1)\n",
        "                    \n",
        "                    max_vals, max_idx = torch.max(outputs, 1)\n",
        "    \n",
        "                    check = torch.sum(max_idx==labels)\n",
        "                    running_acc_m1 += check\n",
        "                    \n",
        "                if mode=='train':\n",
        "                    print(f'Training Loss model 1: {running_loss_m1:.3f} Training Accuracy model 1: {(100*running_acc_m1/dataset_sizes[mode]):.2f}%')\n",
        "                    print(f'Training Loss model 2: {running_loss_m2:.3f}')\n",
        "                else:\n",
        "                    print(f'Validation Loss model 1: {running_loss_m1:.3f} Validation Accuracy model 1: {(100*running_acc_m1/dataset_sizes[mode]):.2f}%')\n",
        "                    print(f'Validation Loss model 2: {running_loss_m2:.3f}')\n",
        "                \n",
        "            print(f'-----------------------------------')\n",
        "           \n",
        "\n",
        "        end = time.time()\n",
        "        training_time = end-start\n",
        "        print(f'Training Completed in: {training_time//60} min {training_time%60:.2f} sec')\n",
        "        print('Finished Training')\n",
        "        return self.model1,self.model2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTExm4vN5QiR"
      },
      "source": [
        "Loss1 and Optimizer1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioVhB0nw_Qlc"
      },
      "outputs": [],
      "source": [
        "criterion_1 = nn.CrossEntropyLoss()\n",
        "optimizer_1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0p4gpE6yjSN"
      },
      "source": [
        "Loss2 and Optimizer2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJfqHMQAyXa2"
      },
      "outputs": [],
      "source": [
        "criterion_2 = nn.MSELoss()  # loss function\n",
        "optimizer_2 = optim.Adam(net2.parameters(), lr=1e-2)  # optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br9u3SNywluY"
      },
      "source": [
        "Train Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69bSl3Yo_aPX"
      },
      "outputs": [],
      "source": [
        "EPOCHS_1 = 10\n",
        "method_1 = Methods()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRMHEhzjzBrA"
      },
      "outputs": [],
      "source": [
        "EPOCHS_2 = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_1,trained_model_2 = method_1.train(net1.to(device),net2.to(device), EPOCHS_1,EPOCHS_2, optimizer_1, optimizer_2,criterion_1 , criterion_2)"
      ],
      "metadata": {
        "id": "0fWmBrVcK_G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ_b35v71L9u"
      },
      "source": [
        "Save Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbrdaV7Z1LEh"
      },
      "outputs": [],
      "source": [
        "FILE_PATH_1 = '/content/gdrive/My Drive/Mobisy/trainedModels/mnist_image_classification_model.pt'\n",
        "torch.save(trained_model_1.state_dict(), FILE_PATH_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA7PxdgM1bOH"
      },
      "source": [
        "Save Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xeFS2aq1ep8"
      },
      "outputs": [],
      "source": [
        "FILE_PATH_2 = '/content/gdrive/My Drive/Mobisy/trainedModels/summation_model.pt'\n",
        "torch.save(trained_model_2.state_dict(), FILE_PATH_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CNmHFijRABQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}